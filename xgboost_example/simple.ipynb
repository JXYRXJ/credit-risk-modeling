{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3721bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    balanced_accuracy_score, average_precision_score, confusion_matrix,\n",
    "    matthews_corrcoef, cohen_kappa_score, log_loss\n",
    ")\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd72f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (26064, 69)\n",
      "Test set: (6517, 69)\n",
      "Class distribution: {np.int64(0): np.int64(25473), np.int64(1): np.int64(7108)}\n",
      "Scale pos weight for XGBoost: 0.28\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed datasets\n",
    "X_train = pd.read_csv('X_train_processed.csv')\n",
    "X_test = pd.read_csv('X_test_processed.csv') \n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution: {dict(zip(*np.unique(np.concatenate([y_train, y_test]), return_counts=True)))}\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "scale_pos_weight = class_weights[0] / class_weights[1]\n",
    "print(f\"Scale pos weight for XGBoost: {scale_pos_weight:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970a0ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "Training Random Forest...\n",
      "‚úÖ Models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "models = {}\n",
    "\n",
    "# XGBoost with exact same parameters\n",
    "models['XGBoost'] = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=5,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.2,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=600,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    max_delta_step=1,\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Random Forest with exact same parameters\n",
    "models['Random Forest'] = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced',\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train models\n",
    "print(\"Training XGBoost...\")\n",
    "models['XGBoost'].fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "models['Random Forest'].fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4bc6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Calculate all performance metrics\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "    logloss = log_loss(y_true, y_prob)\n",
    "    gini = 2 * roc_auc - 1\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'npv': npv,\n",
    "        'fpr': fpr,\n",
    "        'fnr': fnr,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'mcc': mcc,\n",
    "        'kappa': kappa,\n",
    "        'gini': gini,\n",
    "        'log_loss': logloss,\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8307c872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Threshold Tuning:\n",
      "Threshold: 0.10 | Recall: 0.9880 | Precision: 0.3222 | F1: 0.4860\n",
      "Threshold: 0.15 | Recall: 0.9726 | Precision: 0.3701 | F1: 0.5362\n",
      "Threshold: 0.20 | Recall: 0.9578 | Precision: 0.4215 | F1: 0.5854\n",
      "Threshold: 0.25 | Recall: 0.9416 | Precision: 0.4791 | F1: 0.6350\n",
      "Threshold: 0.30 | Recall: 0.9163 | Precision: 0.5356 | F1: 0.6760\n",
      "Threshold: 0.35 | Recall: 0.8966 | Precision: 0.5955 | F1: 0.7157\n",
      "Threshold: 0.40 | Recall: 0.8734 | Precision: 0.6540 | F1: 0.7480\n",
      "Threshold: 0.45 | Recall: 0.8502 | Precision: 0.7054 | F1: 0.7710\n",
      "Threshold: 0.50 | Recall: 0.8270 | Precision: 0.7592 | F1: 0.7917\n",
      "Threshold: 0.55 | Recall: 0.8052 | Precision: 0.8121 | F1: 0.8086\n",
      "Threshold: 0.60 | Recall: 0.7862 | Precision: 0.8607 | F1: 0.8218\n",
      "Threshold: 0.65 | Recall: 0.7679 | Precision: 0.8973 | F1: 0.8276\n",
      "Threshold: 0.70 | Recall: 0.7489 | Precision: 0.9253 | F1: 0.8278\n",
      "Threshold: 0.75 | Recall: 0.7293 | Precision: 0.9505 | F1: 0.8253\n",
      "Threshold: 0.80 | Recall: 0.7194 | Precision: 0.9761 | F1: 0.8283\n",
      "Threshold: 0.85 | Recall: 0.7089 | Precision: 0.9882 | F1: 0.8256\n",
      "\n",
      "‚úÖ Best Threshold: 0.45 | Recall: 0.8502\n"
     ]
    }
   ],
   "source": [
    "# XGBoost threshold optimization\n",
    "print(\"XGBoost Threshold Tuning:\")\n",
    "xgb_prob = models['XGBoost'].predict_proba(X_test)[:, 1]\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "best_thresh = 0.5\n",
    "best_recall = 0\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (xgb_prob >= t).astype(int)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh)\n",
    "    f1s = f1_score(y_test, y_pred_thresh)\n",
    "    print(f\"Threshold: {t:.2f} | Recall: {rec:.4f} | Precision: {prec:.4f} | F1: {f1s:.4f}\")\n",
    "    if rec > best_recall and prec >= 0.7:\n",
    "        best_recall = rec\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"\\n‚úÖ Best Threshold: {best_thresh:.2f} | Recall: {best_recall:.4f}\")\n",
    "\n",
    "# Use optimized threshold for XGBoost\n",
    "xgb_pred = (xgb_prob >= best_thresh).astype(int)\n",
    "xgb_metrics = calculate_comprehensive_metrics(y_test, xgb_pred, xgb_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e3f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest evaluation (no threshold tuning)\n",
    "rf_pred = models['Random Forest'].predict(X_test)\n",
    "rf_prob = models['Random Forest'].predict_proba(X_test)[:, 1]\n",
    "rf_metrics = calculate_comprehensive_metrics(y_test, rf_pred, rf_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8895922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Detailed Performance Metrics:\n",
      "--------------------------------------------------\n",
      "accuracy                  0.8898    \n",
      "balanced_accuracy         0.8755    \n",
      "precision                 0.7054    \n",
      "recall                    0.8502    \n",
      "specificity               0.9009    \n",
      "f1_score                  0.7710    \n",
      "npv                       0.9557    \n",
      "fpr                       0.0991    \n",
      "fnr                       0.1498    \n",
      "roc_auc                   0.9507    \n",
      "pr_auc                    0.9068    \n",
      "mcc                       0.7046    \n",
      "kappa                     0.6993    \n",
      "gini                      0.9013    \n",
      "log_loss                  0.2497    \n",
      "\n",
      "============================================================\n",
      "Random Forest - Detailed Performance Metrics:\n",
      "--------------------------------------------------\n",
      "accuracy                  0.9050    \n",
      "balanced_accuracy         0.8490    \n",
      "precision                 0.8021    \n",
      "recall                    0.7496    \n",
      "specificity               0.9484    \n",
      "f1_score                  0.7750    \n",
      "npv                       0.9314    \n",
      "fpr                       0.0516    \n",
      "fnr                       0.2504    \n",
      "roc_auc                   0.9225    \n",
      "pr_auc                    0.8702    \n",
      "mcc                       0.7155    \n",
      "kappa                     0.7149    \n",
      "gini                      0.8451    \n",
      "log_loss                  0.2955    \n"
     ]
    }
   ],
   "source": [
    "# XGBoost Results\n",
    "print(\"XGBoost - Detailed Performance Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "for key, val in xgb_metrics.items():\n",
    "    if key not in ['tp','tn','fp','fn']:\n",
    "        print(f\"{key:<25} {val:<10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Random Forest Results\n",
    "print(\"Random Forest - Detailed Performance Metrics:\")  \n",
    "print(\"-\" * 50)\n",
    "for key, val in rf_metrics.items():\n",
    "    if key not in ['tp','tn','fp','fn']:\n",
    "        print(f\"{key:<25} {val:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec1f7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç COMPREHENSIVE SHAP EXPLAINABILITY ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m sns.set_palette(\u001b[33m\"\u001b[39m\u001b[33mhusl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Sample data for faster SHAP computation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m sample_size = \u001b[38;5;28mmin\u001b[39m(\u001b[32m1000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mX_test\u001b[49m))\n\u001b[32m     19\u001b[39m X_test_sample = X_test.sample(n=sample_size, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     20\u001b[39m y_test_sample = y_test[X_test_sample.index]\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# ================================\n",
    "# üîç SHAP EXPLAINABILITY ANALYSIS\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç COMPREHENSIVE SHAP EXPLAINABILITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Sample data for faster SHAP computation\n",
    "sample_size = min(1000, len(X_test))\n",
    "X_test_sample = X_test.sample(n=sample_size, random_state=42)\n",
    "y_test_sample = y_test[X_test_sample.index]\n",
    "\n",
    "print(f\"Analyzing {sample_size} test samples for explainability...\")\n",
    "\n",
    "# ================================\n",
    "# üìä XGBOOST SHAP ANALYSIS\n",
    "# ================================\n",
    "\n",
    "print(\"\\nüöÄ XGBoost SHAP Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# XGBoost TreeExplainer\n",
    "xgb_explainer = shap.TreeExplainer(models['XGBoost'])\n",
    "xgb_shap_values = xgb_explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(\"‚úÖ XGBoost SHAP values calculated!\")\n",
    "\n",
    "# 1. XGBoost Feature Importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(xgb_shap_values, X_test_sample, plot_type=\"bar\", show=False, max_display=15)\n",
    "plt.title(\"XGBoost - SHAP Feature Importance\", fontsize=14, pad=20)\n",
    "plt.xlabel(\"Mean |SHAP Value|\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('xgb_shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. XGBoost Summary Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(xgb_shap_values, X_test_sample, show=False, max_display=15)\n",
    "plt.title(\"XGBoost - SHAP Feature Impact Distribution\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('xgb_shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
